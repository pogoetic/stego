{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"NEXT STEPS:\n",
    "\n",
    "DATA:\n",
    "1. STORE HISTORICAL DATA\n",
    "    a. Get recession dates loaded\n",
    "    b. Convert all series to daily (generic function)\n",
    "    https://stackoverflow.com/questions/19324453/add-missing-dates-to-pandas-dataframe\n",
    "    c. Adjust for inflation where necessary (generic function)\n",
    "2. DATA UPDATE PROCESS\n",
    "    a. daily check for each dataset\n",
    "    b. repeat steps 1b/1c\n",
    "    c. email alerts if data update fails\n",
    "\n",
    "MODEL:\n",
    "3. Feature Engineering (trended attributes, etc..)\n",
    "4. Employ Prophet for TimeSeries modelling\n",
    "    a. 1 Model with wide prediction range (3mo)\n",
    "    b. 1 Model with short perdiction range (1mo)\n",
    "5. Re-run 'best' models every day and check against prior scores\n",
    "\n",
    "ALERTS:\n",
    "6. Email Alerts if model scores change within a tolerance\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Goal: To build a basic economic model for the US Economy in order to roughly predict \n",
    "a downturn. Prob(downturn in 3 months). \n",
    "\n",
    "Steps: \n",
    "Gather quality data in time-series\n",
    "Train/Test model\n",
    "Automate from the outset so model can ingest new data and re-run daily\n",
    "Alert if target probability exceeds alert threshold\n",
    "\n",
    "Data sources:\n",
    "FRED Data (st louis federal reserve) - https://fred.stlouisfed.org/\n",
    "FRED API - https://github.com/mortada/fredapi\n",
    "List of other datasets: https://www.aeaweb.org/rfe/showCat.php?cat_id=3\n",
    "\n",
    "Indicators we care about:\n",
    "Private Debt, Household Debt vs. GDP\n",
    "Personal Income\n",
    "Unemployment Rate\n",
    "Interest rates\n",
    "Monetary Supply\n",
    "CPI\n",
    "GDP\n",
    "Public Debt\n",
    "Credit in the economy\n",
    "VIX index\n",
    "Total US Population\n",
    "YieldCurve/Treasury spreads (2-10yr bonds)\n",
    "S&P500 - Measure of total us stock market prices(target)\n",
    "Charge Off Rates - FED published for top banks, other source as well\n",
    "\"\"\"\n",
    "\n",
    "import json, sqlite3, os, sys, datetime, urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fredapikey import apikey\n",
    "from fredapi import Fred\n",
    "from functools import reduce\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "verbose = 1\n",
    "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "dbpathname = '/stego.db'\n",
    "fred = Fred(api_key=apikey)\n",
    "\n",
    "def echo(msg, verbosity=verbose):\n",
    "    if verbosity == 1:\n",
    "        print(msg) \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def dbprocess(path):\n",
    "    try:\n",
    "        echo(msg='Loading DB...',verbosity=1)\n",
    "        con = sqlite3.connect(str(path)+dbpathname) #connect to existing or create new if does not exist\n",
    "        cur = con.cursor()\n",
    "        cur.execute('CREATE TABLE daily_trades([index] bigint, asset varchar(5), exchange varchar(100), time_start datetime, time_end datetime, trades_count bigint, volume_traded decimal(20,10), price_open decimal(20,10), price_high decimal(20,10), price_low decimal(20,10), price_close decimal(20,10))')\n",
    "        #cur.execute('CREATE INDEX IDX_time ON daily_trades (time_start, time_end)')\n",
    "        con.commit()\n",
    "    except sqlite3.Error as e:\n",
    "        echo(msg=\"Error {}:\".format(e.args[0]),verbosity=1)\n",
    "        os._exit(1)\n",
    "    finally:\n",
    "        if con:\n",
    "            con.close()\n",
    "\n",
    "def dailyresample(data, seriesname):\n",
    "    #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html\n",
    "    #https://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n",
    "    if isinstance(data, pd.Series):\n",
    "        s = data.resample('D').pad()\n",
    "        df = s.to_frame(name=seriesname)\n",
    "        df.index.name='date'\n",
    "        return df\n",
    "    else:\n",
    "        print('not a pandas series - need code')\n",
    "        raise\n",
    "    \n",
    "\n",
    "#Create or Connect to existing Sqlite DB\n",
    "if not os.path.isfile(str(dir_path)+dbpathname):\n",
    "    con=dbprocess(path=dir_path)\n",
    "\n",
    "con = sqlite3.connect(str(dir_path)+dbpathname) #connect to existing sqlite db\n",
    "cur = con.cursor()\n",
    "\n",
    "#Historical Data Load\n",
    "#   Recession Dates\n",
    "dtype = {'c':str,'d':str,'e':int,'f':int,'g':int,'h':int,'i':int,'j':int}\n",
    "rcs_df = pd.read_excel(io=dir_path+\"/required/NBER chronology.xlsx\",\n",
    "                    sheet_name=0,\n",
    "                    header=2,\n",
    "                    usecols=\"C:J\",\n",
    "                    dtype=dtype,\n",
    "                    nrows=34)\n",
    "rcs_df['Peak month'] = rcs_df['Peak month'].apply(lambda x: '01 {}'.format(x))\n",
    "rcs_df['Trough month'] = rcs_df['Trough month'].apply(lambda x: '01 {}'.format(x))\n",
    "df_dates = pd.to_datetime(rcs_df[['Peak month','Trough month']].stack(),errors='coerce',format='%d %B %Y').unstack()\n",
    "rcs_df = pd.merge(rcs_df, df_dates, left_index=True, right_index=True)[['Peak month_y','Trough month_y','Peak month number','Trough month number','Duration, peak to trough','Duration, trough to peak','Duration, trough to trough','Duration, peak to peak']]\n",
    "rcs_df.rename(index=str, columns={'Peak month_y': 'Peak month','Trough month_y': 'Trough month'}, inplace=True)\n",
    "print(rcs_df)\n",
    "\n",
    "#   S&P500 History (Macrotrends data from Dec-1927)\n",
    "#   https://macrotrends.dpdcart.com/product/126227\n",
    "sp_df = pd.read_csv(filepath_or_buffer=dir_path+\"/required/Macrotrends-s-p-500-index-daily.csv\",\n",
    "                    delimiter=',',\n",
    "                    skiprows=8,\n",
    "                    header=0,\n",
    "                    usecols=[0,1],\n",
    "                    dtype=dtype)\n",
    "df_dates = pd.to_datetime(sp_df[['Date']].stack(),errors='coerce',infer_datetime_format=True).unstack()\n",
    "sp_df = pd.merge(sp_df, df_dates, left_index=True, right_index=True)[['Date_y','Closing Value']]\n",
    "sp_df.rename(index=str, columns={'Date_y': 'date','Closing Value': 'SP500'}, inplace=True)\n",
    "sp_df = sp_df.interpolate() #fill NaN values unsing linear model - averaging with nearest neighbors\n",
    "sp_df = sp_df.set_index('date')\n",
    "sp_df = dailyresample(data=sp_df['SP500'],seriesname='SP500')\n",
    "\n",
    "#   S&P500 History (Shiller simulated monthly data from Jan-1871)\n",
    "#   http://www.econ.yale.edu/~shiller/data/ie_data.xls\n",
    "urllib.request.urlretrieve('http://www.econ.yale.edu/~shiller/data/ie_data.xls','./required/ie_data.xls')\n",
    "shiller_df = pd.read_excel(io=dir_path+\"/required/ie_data.xls\",\n",
    "                            sheet_name=\"Data\",\n",
    "                            header=7,\n",
    "                            usecols=\"A:B,K\",\n",
    "                            nrows=1773)\n",
    "shiller_df['Date'] = shiller_df['Date'].apply(lambda x: '{}0'.format(x)[:7]+'.01')\n",
    "df_dates = pd.to_datetime(shiller_df[['Date']].stack(),errors='raise',infer_datetime_format=True).unstack()\n",
    "shiller_df = pd.merge(shiller_df, df_dates, left_index=True, right_index=True)[['Date_y','P','CAPE']]\n",
    "shiller_df.rename(index=str, columns={'Date_y': 'Date'}, inplace=True)\n",
    "shiller_df = shiller_df.set_index('Date')\n",
    "shiller_sp500 = dailyresample(data=shiller_df['P'],seriesname='SP500')\n",
    "shiller_sp500 = shiller_sp500.loc[:'1927-12-29'] #remove dates that overlap with our other datasets\n",
    "\n",
    "#   S&P500 History from FRED goes back to 2008\n",
    "SP500 = fred.get_series('SP500')\n",
    "SP500 = dailyresample(data=SP500,seriesname='SP500')\n",
    "SP500 = SP500.loc['2018-10-12':]\n",
    "\n",
    "SP500=pd.concat([shiller_sp500,sp_df,SP500]) #union all our normalized SP500 data. \n",
    "\n",
    "\n",
    "#Trend the S&P by 6mo, 12mo, 24mo. Compare current to prior highs from these periods. \n",
    "#Calc 'months before recession' feature (could be our target)\n",
    "\n",
    "\n",
    "\n",
    "#CAPE index by Shiller\n",
    "shiller_cape = dailyresample(data=shiller_df['CAPE'],seriesname='CAPE')\n",
    "shiller_cape = shiller_cape.loc['1881-01-01':] #remove NaN dates\n",
    "#print(shiller_cape.tail())\n",
    "\n",
    "VIXCLS = fred.get_series('VIXCLS') #CBOE Volatility Index (VIX), Daily\n",
    "VIXCLS = dailyresample(data=VIXCLS,seriesname='VIXCLS')\n",
    "\n",
    "UMCSENT = fred.get_series('UMCSENT') # University of Michigan: Consumer Sentiment, Monthly\n",
    "UMCSENT = dailyresample(data=UMCSENT,seriesname='UMCSENT')\n",
    "\n",
    "STLFSI = fred.get_series('STLFSI') # St. Louis Fed Financial Stress Index, Weekly\n",
    "STLFSI = dailyresample(data=STLFSI,seriesname='STLFSI')\n",
    "#print(STLFSI.tail())\n",
    "\n",
    "NPPTTL = fred.get_series('NPPTTL') # Total Nonfarm Private Payroll Employment, Monthly\n",
    "NPPTTL = dailyresample(data=NPPTTL,seriesname='NPPTTL')\n",
    "#print(NPPTTL.tail())\n",
    "\n",
    "PERMIT = fred.get_series('PERMIT') # New Private Housing Units Authorized by Building Permits, Monthly\n",
    "PERMIT = dailyresample(data=PERMIT,seriesname='PERMIT')\n",
    "#print(PERMIT.tail())\n",
    "\n",
    "DGORDER = fred.get_series('DGORDER') # Manufacturers' New Orders: Durable Goods, Seasonally Adjusted, Monthly\n",
    "DGORDER = dailyresample(data=DGORDER,seriesname='DGORDER')\n",
    "#print(DGORDER.tail())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The first indicator is the unemployment gap, which is the difference between the unemployment rate and the natural rate of unemployment (formerly called NAIRU, for the non-accelerating inflation rate of unemployment). \n",
    "A strong labor market prompts the Fed to tighten because an unemployment rate well below the natural rate is unsustainable by definition, and can lead to a spike in wage and price inflation. \n",
    "Looking at the current cycle, the labor market is in the early stages of overheating. We see unemployment heading to 3.5 percent, which would be consistent with the pre-recession behavior of the unemployment gap in past cycles.\n",
    "\"\"\"\n",
    "#Unemployment Gap  = Unemployment Rate – Natural Rate of Unemployment\n",
    "NROU = fred.get_series('NROU') # Natural Rate of Unemployment (Long-Term), Quarterly\n",
    "NROU = dailyresample(data=NROU,seriesname='NROU')\n",
    "#print(NROU.tail())\n",
    "\n",
    "NROUST = fred.get_series('NROUST') # Natural Rate of Unemployment (Short-Term), Quarterly\n",
    "NROUST = dailyresample(data=NROUST,seriesname='NROUST')\n",
    "#print(NROUST.tail())\n",
    "\n",
    "\"\"\"\n",
    "One of the most reliable and consistent predictors of recession has been the Treasury yield curve. Recessions are always preceded by a flat or inverted yield curve, usually occurring about 12 months before the downturn begins. \n",
    "This occurs with T-bill yields rising as Fed policy becomes restrictive while 10-year yields rise at a slower pace. Looking at the current cycle, we expect that steady increases in the fed funds rate will continue to flatten the yield curve over the next 12–18 months.\n",
    "Three Month–10 Year Treasury Yield Curve (bps)\n",
    "\"\"\"\n",
    "T10Y2Y = fred.get_series('T10Y2Y') # 10-Year Treasury Constant Maturity Minus 2-Year Treasury Constant Maturity, Daily\n",
    "T10Y2Y = dailyresample(data=T10Y2Y,seriesname='T10Y2Y')\n",
    "#print(T10Y2Y.tail())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Other indicators of the real economy, including aggregate weekly hours, decline in the months preceding a recession as employers begin to reduce headcount and cut the length of the workweek. \n",
    "Looking at the current cycle, aggregate weekly hours growth has been steady, albeit at weaker than average levels, reflecting slower labor force growth as baby boomers retire. \n",
    "We expect growth in hours worked to hold up over the coming year before slowing more markedly in 2019.\n",
    "Aggregate Weekly Hours Worked, Year-over-Year % Change\n",
    "\"\"\"\n",
    "HOHWMN02USM065S = fred.get_series('HOHWMN02USM065S') # Weekly Hours Worked: Manufacturing for the United States, Monthly\n",
    "HOHWMN02USM065S = dailyresample(data=HOHWMN02USM065S,seriesname='HOHWMN02USM065S')\n",
    "#print(HOHWMN02USM065S.tail())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Real retail sales growth weakens significantly before a recession begins, with the inflection point typically occurring about 12 months before the start of the recession. \n",
    "Consumers cut back on spending as they start to feel the impact of slowing real income growth. This shows up most noticeably in retail sales, which are made up of a higher share of discretionary purchases than other measures of consumption. \n",
    "Looking at the current cycle, real retail sales growth has been steady at around 2 percent. This is weaker than the historical average, but is consistent with slower-trend gross domestic product (GDP) growth in this cycle.\n",
    "\"\"\"\n",
    "#Need to adjust for inflation to get Real\n",
    "RETAILSMSA = fred.get_series('RETAILSMSA') # Retailers Sales, Seasonally Adjusted, Monthly\n",
    "RETAILSMSA = dailyresample(data=RETAILSMSA,seriesname='RETAILSMSA')\n",
    "#print(RETAILSMSA.tail())\n",
    "\n",
    "\n",
    "#data = fred.search('yield').T\n",
    "#print(data)\n",
    "\n",
    "#Pull Economic Uncertainty Index\n",
    "#https://www.sydneyludvigson.com/data-and-appendixes/\n",
    "#https://www.sydneyludvigson.com/s/MacroFinanceUncertainty_2018AUG_update.zip\n",
    "\n",
    "#Scrape LEI data from web (auto or manual)\n",
    "#https://www.conference-board.org/data/bciarchive.cfm?cid=1\n",
    "\n",
    "\n",
    "#Construct main dataframe indexed on our dates\n",
    "start_date=datetime.date(1854,12,1)\n",
    "end_date=datetime.datetime.today().date()\n",
    "days=(end_date-start_date).days\n",
    "index = pd.date_range(start=start_date, end=end_date)\n",
    "s = pd.Series(range(days+1), index=index)\n",
    "main_df = s.to_frame(name='daynum')\n",
    "\n",
    "#Merge all dataframes into 1 dataset\n",
    "#https://stackoverflow.com/questions/23668427/pandas-three-way-joining-multiple-dataframes-on-columns\n",
    "dfs = [main_df,SP500,VIXCLS,UMCSENT,STLFSI,NPPTTL,PERMIT,DGORDER,NROU,NROUST,T10Y2Y,HOHWMN02USM065S,RETAILSMSA]\n",
    "main_df = reduce(lambda left,right: pd.merge(left,right,how='outer',left_index=True,right_index=True), dfs)\n",
    "\n",
    "print(main_df[51000:].head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Additional predictors here:\n",
    "#https://cdn.titanvest.com/library/Titan_State_of_Markets.pdf\n",
    "#Stock buyback activity, stock earnings growth, level of corp profits, \n",
    "#Leading indicators, such as growth in industrial transports, remain healthy\n",
    "# P/E multiples vs. interest rates and inflation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Subtracting the natural rate of interest—which is the neutral fed funds rate, neither contractionary nor stimulative for the economy—from the real fed funds rate gives us a gauge of how loose or tight Fed policy is. \n",
    "Leading up to past recessions, the Fed has usually hiked rates beyond the natural rate to cool the labor market and get ahead of inflation, only to inadvertently push the economy into recession. \n",
    "Looking at the current cycle, we expect quarterly rate hikes to resume in December. This will put Fed policy well into restrictive territory next year, barring a sharper increase in the natural rate than we expect.\n",
    "*Natural rate is Laubach-Williams one-sided filtered estimate.\n",
    "Real Fed Funds Rate – Natural Rate of Interest (r*)\n",
    "\n",
    "https://www.frbsf.org/economic-research/files/el2017-16.pdf\n",
    "Starting with a view of longer-run trends, Figure 1 plots the inflation-adjusted or “real” federal funds rate,\n",
    "computed as the nominal federal funds rate minus the trailing four-quarter core personal consumption\n",
    "expenditures (PCE) inflation rate. Core inflation tends to be a better predictor of future inflation because it\n",
    "removes the volatile food and energy components. \n",
    "\n",
    "Might be able to use Potential GDP Growth as a stand-in for proper Laubach-Williams r* model\n",
    "\n",
    "data = fred.get_series('DFF') # Effective Federal Funds Rate, Daily\n",
    "print(data.tail())\n",
    "\n",
    "#use PCE to adjust for inflation\n",
    "data = fred.get_series('PCEPILFE') # Personal Consumption Expenditures Excluding Food and Energy (Chain-Type Price Index), Monthly\n",
    "print(data.tail())\n",
    "\n",
    "#May need for calculation of r*\n",
    "data = fred.get_series('GDPPOT') # Real Potential Gross Domestic Product, Quarterly\n",
    "print(data.tail())\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"NOTES:\n",
    "\n",
    "Paul Comments: \n",
    "There is some work on this already (check Philly Fed and St louis Fed working papers).  I looked at these recently, highlighted the most important in red. I personally feel it is in the following order:\n",
    "1.  10-2 yr spread (60%)\n",
    "2.  S&P500 vs recent(last 12 or 24mo) peak/high (20%)    \n",
    "3.  Change in Housing permits (10%)\n",
    "4.  Change in Manufacturing Orders (5%)\n",
    "5.  Consumer Sentiment (5%)\n",
    "\n",
    "•   Manufacturing survey….leading indicator. Given lead times to build, this is historically a leading indicator. It was more pronounce when we were more of a manufacturing based society with less global influence but still predictive.\n",
    "•   Housing Permits….. leading indicator given lead time to build. Also note interest rate increases slow consumer demand and construction companies know this. Somewhat correlated to yield curve.\n",
    "•   Consumer sentiment….(mich survey) …leading indicator however I feel the S&P500 is a reliable gauge for this and more real time. The two series are highly correlated.\n",
    "\n",
    "JPMorgan's proprietary model considers the levels of several economic indicators, including consumer sentiment, manufacturing sentiment, building permits, auto sales, and unemployment.\n",
    "JPMorgan notes that nonfarm payrolls is actually not part of the model. But the unemployment rate is. Interestingly, a low unemployment rate can be considered an ominous sign.\n",
    "\n",
    "“The unemployment rate enters the model in two ways,\" Edgerton explained. \"As a near-term indicator, we watch for increases in the unemployment rate that occur near the beginning of recessions. So this morning's move down in the unemployment rate lowered the recession probability in our near-term model. But we also find the level of the unemployment rate to be one of the most useful indicators of medium-term recession risk. So the move down in unemployment raises the model's view of the risk of economic overheating in the medium run and raises the 'background risk' of recession.\"\n",
    "\n",
    "Indeed, recessions begin when things are very good. It's only when reports come in that the data has turned that we realize we've been in a recession.\n",
    "https://finance.yahoo.com/news/jpmorgan-recession-risk-new-high-160251309.html\n",
    "\n",
    "Best investments in case of an inverted yield curve\n",
    "https://www.wsj.com/articles/the-best-investments-in-case-of-an-inverted-yield-curve-1536545041?emailToken=e7d3ad6442c26d159eb5bc7903c7f80603yQYaTVNumvrlRPOpnMOdMNGYlKz9R0nF7hBawD18PIgfIs2FyveE1L1m8yYL2DkzsrmGL+Gt0ehEfETb7Q1TP6do5LV1+ScyBBPIcgjOqLKKtUfjtFBaeAgRiUo7m2&reflink=article_email_share\n",
    "\n",
    "Guggenheim Models:\n",
    "https://www.guggenheiminvestments.com/perspectives/macroeconomic-research/forecasting-the-next-recession\n",
    "https://www.guggenheiminvestments.com/perspectives/macroeconomic-research/updating-our-outlook-for-recession-timing\n",
    "Includes cycles ending in 1970, 1980, 1990, 2001, and 2007.\n",
    "\n",
    "Paul O. - What to do just before recession: \n",
    "Between now and 12 mos from now, bond prices will have pressure so interest rate hedge ETF could better navigate this. Then swap them out 13mos (bypass short term cap gains tax) from now for non hedge versions so you can gain from price increases associate with rate decreases.  Just need to watch out for credit risk in order:\n",
    "1.  Junk\n",
    "2.  Investment grade Corp\n",
    "3.  Muni\n",
    "\n",
    "Paul O. - Economist from Moody's last week said the earliest predictor of a recession is when UE falls below the natural rate of unemployment. This lead is 3yrs, it happened June-2017.\n",
    "\n",
    "####OFFICIAL RECESSION DATES\n",
    "https://www.nber.org/cycles.html\n",
    "\n",
    "#Natural rate of interest modelling: \n",
    "https://www.frbsf.org/economic-research/files/wp2016-11.pdf\n",
    "https://www.frbsf.org/economic-research/files/el2017-16.pdf\n",
    "\n",
    "#Economic Uncertainty Index (increases sharply just before recession) (not to be confused with Economic Policy Uncertainty)\n",
    "https://www.sydneyludvigson.com/data-and-appendixes/\n",
    "https://www.sydneyludvigson.com/s/MacroFinanceUncertainty_2018AUG_update.zip\n",
    "\n",
    "#Leading Economic Index (LEI)\n",
    "The Conference Board Leading Economic Index (LEI), which measures 10 key variables, is itself a recession predictor, albeit a fallible one. \n",
    "It has been irreverently said that the LEI predicted 15 out of the last eight recessions. \n",
    "Nevertheless, growth in the LEI always slows on a year-over-year basis heading into a recession, and turns negative about seven months out, on average. \n",
    "Looking at the current cycle, LEI growth of 4 percent over the past year has been on par with past cycles two years before a recession, and we will be watching for a deceleration over the course of the coming year.\n",
    "Leading Economic Index, Year-over-Year % Change\n",
    "\n",
    "https://www.conference-board.org/data/datasearch.cfm?cid=1&output=2&sector=BCI-01A%20-%20Composite%20Indexes-Leading%20Economic%20Indicators\n",
    "#Could scrape the index from their public releases:\n",
    "https://www.conference-board.org/data/bcicountry.cfm?cid=1\n",
    "\n",
    "###USE FACEBOOK PROPHET FOR TIME SERIES MODEL\n",
    "http://www.degeneratestate.org/posts/2017/Jul/24/making-a-prophet/\n",
    "https://facebook.github.io/prophet/docs/quick_start.html#python-api\n",
    "https://peerj.com/preprints/3190.pdf\n",
    "\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
